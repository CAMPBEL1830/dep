import pandas as pd

# Supposons que votre DataFrame s'appelle 'data'

# Pour chaque colonne numérique du DataFrame
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    
    # Définir les bornes pour détecter les outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Filtrer les outliers pour la colonne actuelle
    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

# Afficher le DataFrame après suppression des outliers
print(data)



columns = ['colonne1', 'colonne2', 'colonne3']  # Remplacez par vos colonnes

for col in columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

# Afficher le DataFrame après suppression des outliers
print(data)





import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Extraction des coefficients et calcul de l'importance des features
coefficients = model.coef_[0]
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': coefficients
})
feature_importances['Importance'] = np.abs(feature_importances['Coefficient'])
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Visualisation des importances des features
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Importance des Features (Régression Logistique)')
plt.show()
